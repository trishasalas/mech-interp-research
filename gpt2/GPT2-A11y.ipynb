{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec37119-3bbe-4b3f-949b-7f628a2578a9",
   "metadata": {},
   "source": [
    "# GPT2 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ae3178-107d-46d6-b5d8-efb51b14e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Loaded: GPT-2 small\n",
      "Layers: 12\n",
      "Heads: 12\n",
      "Hidden size: 768\n",
      "Params: 163.0M\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39f3530-c5b8-45ae-9bac-e7e9d17cb7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18557ccc35647cca86c3a958c6acf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A screen reader is\n",
      "Output: A screen reader is a device that reads and writes data to a screen\n",
      "\n",
      "Cached 208 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A screen reader is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8e5a81-361f-429a-a89a-bc4ba968467e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25bc8b531a1443e8fc5ad6acc837f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: WCAG stands for\n",
      "Output: WCAG stands for \"Western Hemisphere Association of Chiefs of Police.\" It\n",
      "\n",
      "Cached 208 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"WCAG stands for\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c091425b-02d7-4528-86c7-b85aea0369e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19312ffe650d405b9ee26d63af061707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A skip link is\n",
      "Output: A skip link is a link to a page that is not available.\n",
      "\n",
      "Cached 208 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A skip link is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "150b5bd7-f648-4f98-a6af-0c7af74f8f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a137bf971b40bba4d81480a5e97a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The purpose of alt text is\n",
      "Output: The purpose of alt text is to provide a simple way to create a simple,\n",
      "\n",
      "Cached 208 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The purpose of alt text is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1511e90c-bda3-499b-acfd-8441da516909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Run this between models\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc54cd8-e83b-4a1d-989c-e3a9e14cc946",
   "metadata": {},
   "source": [
    "# GPT2 Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d67e6f01-3ad1-459c-9f06-caa11fb707a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "Loaded: GPT-2 medium\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-medium\")\n",
    "print(f\"Loaded: GPT-2 medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03ab555c-3da6-4a01-8a7b-5234a60d2d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6601273bd3343e19b1e93cb82509e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A screen reader is\n",
      "Output: A screen reader is a device that allows you to read text on a\n",
      "\n",
      "Cached 412 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A screen reader is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2964a8-0924-498d-b85b-2dbf2053c725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417a8dbfabf74df587782d59e07f556d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: WCAG stands for\n",
      "Output: WCAG stands for World Championship of Hockey. It is a tournament that\n",
      "\n",
      "Cached 412 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"WCAG stands for\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a34f06e-885d-408f-b714-918dffa7a800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bd1b00e7d04b0fa899414d12d9319e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A skip link is\n",
      "Output: A skip link is a link that is not part of the original file\n",
      "\n",
      "Cached 412 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A skip link is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6135d18b-fa1f-4ad0-a902-217649767857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e619d40b770547b8926c6f585cb7a826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The purpose of alt text is\n",
      "Output: The purpose of alt text is to provide a way for users to communicate with each\n",
      "\n",
      "Cached 412 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The purpose of alt text is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e159ed68-2962-4949-ae72-4e7746a2ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Run this between models\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfc0dc-533d-4b8b-a2bb-ef40a03e7be5",
   "metadata": {},
   "source": [
    "# GPT2 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d58f23c-6aa2-42f7-9a7e-5a7193662da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n",
      "Loaded: GPT-2 large\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "# Load GPT-2 large\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-large\")\n",
    "print(f\"Loaded: GPT-2 large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91e745fb-417f-429f-86cf-0d494d650195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38331ea730c43168dfe43f5eb0290a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A screen reader is\n",
      "Output: A screen reader is a device that reads text on a screen. It\n",
      "\n",
      "Cached 616 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A screen reader is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7780d85-4af2-44c9-bd60-bcae6ab08551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332e93c6102b47bb8e625d9a95482441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: WCAG stands for\n",
      "Output: WCAG stands for Women's College American Honor Society. It is a\n",
      "\n",
      "Cached 616 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"WCAG stands for\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a220e82-9356-4882-8c17-3ebcd0980fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debc4d9db5824137909af656f30633a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A skip link is\n",
      "Output: A skip link is a link that is not part of the main article\n",
      "\n",
      "Cached 616 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A skip link is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d64b66c1-c176-4d9b-b9e6-3762c4c29214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c453415ebe5647edb256708dec953f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The purpose of alt text is\n",
      "Output: The purpose of alt text is to provide a way to provide a more readable alternative\n",
      "\n",
      "Cached 616 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The purpose of alt text is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2256fb04-14ad-40af-9c4d-174499463246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Run this between models\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b71f5b-30f3-4798-b14e-1ecaaf1a8601",
   "metadata": {},
   "source": [
    "# GPT2 XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d907dc56-5998-4b17-8213-9e92ad17fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "# Load GPT-2 large\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4930720d-1946-4ea0-9288-71ec033be4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a96d44ebf26494baf3ac3b2c90ccbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The purpose of a screen reader is\n",
      "Output: The purpose of a screen reader is to help you understand what is being said on a\n",
      "\n",
      "Cached 820 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The purpose of a screen reader is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4a59c2-b46c-4b96-ade9-73dbf53f1e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9138efcb16444300a92c532443506418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: WCAG stands for\n",
      "Output: WCAG stands for Web Content Accessibility Guidelines. WCAG is a\n",
      "\n",
      "Cached 820 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"WCAG stands for\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a48aa054-91b0-4996-8e7e-ba7e3fd585a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878fb5191b92462ba048c083cf4326a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A skip link is\n",
      "Output: A skip link is a link that is not a link. It is\n",
      "\n",
      "Cached 820 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A skip link is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5dd84ec-d998-4223-b160-156e43f180de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c162a019094c5287861c3e28e91d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The purpose of alt text is\n",
      "Output: The purpose of alt text is to provide a brief description of the image or text\n",
      "\n",
      "Cached 820 different activation points!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The purpose of alt text is\"\n",
    "output = model.generate(prompt, max_new_tokens=10, temperature=0)\n",
    "print(f\"Input: {prompt}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Cache internal states\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "print(f\"\\nCached {len(cache)} different activation points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdaef07f-790e-4836-b2a7-010b5f914b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Run this between models\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f98d8-5daa-4d57-8d87-bdf4d1390ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
